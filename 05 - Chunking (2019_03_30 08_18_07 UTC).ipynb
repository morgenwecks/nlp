{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the parts of speech, we can do what is called chunking, and **group words into hopefully meaningful chunks**. \n",
    "\n",
    "**One of the main goals of chunking is to group into what are known as \"noun phrases.\"** These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to chunk, we combine the part of speech tags with **regular expressions**. Mainly from regular expressions, we are going to utilize the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or MORE repetitions\t  \n",
    ". = Any character except a new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last things to note is that the **part of speech tags are denoted with the \"<\" and \">\"** and we can also place regular expressions within the tags themselves, so account for things like \"all nouns\" (<N.*>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "%matplotlib inline\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text) #from before: tokenizing sentences, result is a list of words for each sentence\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text) # here I instantiate the test set based on earlier results (just like in other ML algo)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()\n",
    "\n",
    "#careful, will go thru whole speech if not limited to some sort like above variable \"words\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" is broken down as:\n",
    "\n",
    "<RB.?>* = \"0 or more of any tense of adverb,\" followed by:\n",
    "\n",
    "<VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
    "\n",
    "<NNP>+ = \"One or more proper nouns,\" followed by\n",
    "\n",
    "<NN>? = \"zero or one singular noun.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try playing around with combinations to group various instances until you feel comfortable with chunking.\n",
    "\n",
    "Not covered in the video, but also a **reasonable task is to actually access the chunks specifically**. This is something rarely talked about, but can be an essential step depending on what you're doing. Say you print the chunks out, you are going to see output like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(S\n",
    "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
    "  'S/POS\n",
    "  (Chunk\n",
    "    ADDRESS/NNP\n",
    "    BEFORE/NNP\n",
    "    A/NNP\n",
    "    JOINT/NNP\n",
    "    SESSION/NNP\n",
    "    OF/NNP\n",
    "    THE/NNP\n",
    "    CONGRESS/NNP\n",
    "    ON/NNP\n",
    "    THE/NNP\n",
    "    STATE/NNP\n",
    "    OF/NNP\n",
    "    THE/NNP\n",
    "    UNION/NNP\n",
    "    January/NNP)\n",
    "  31/CD\n",
    "  ,/,\n",
    "  2006/CD\n",
    "  THE/DT\n",
    "  (Chunk PRESIDENT/NNP)\n",
    "  :/:\n",
    "  (Chunk Thank/NNP)\n",
    "  you/PRP\n",
    "  all/DT\n",
    "  ./.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that helps us visually, but what if we want to access this data via our program? \n",
    "\n",
    "Well, what is happening here is our \"chunked\" variable is an **NLTK tree**. \n",
    "\n",
    "Each \"chunk\" and \"non chunk\" is a \"subtree\" of the tree. We can reference these by doing something like chunked.subtrees. We can then iterate through these subtrees like so:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for subtree in chunked.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we might be only interested in getting just the chunks, ignoring the rest. We can use the filter parameter in the chunked.subtrees() call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're filtering to only show the subtrees with the label of \"Chunk.\" **Keep in mind, this isn't \"Chunk\" as in the NLTK chunk attribute**... this is \"Chunk\" literally because that's the label we gave it here: chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had we said instead something like chunkGram = r\"\"\"Pythons: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\", then we would filter by the label of \"Pythons.\" The result here should be something like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
    "(Chunk\n",
    "  ADDRESS/NNP\n",
    "  BEFORE/NNP\n",
    "  A/NNP\n",
    "  JOINT/NNP\n",
    "  SESSION/NNP\n",
    "  OF/NNP\n",
    "  THE/NNP\n",
    "  CONGRESS/NNP\n",
    "  ON/NNP\n",
    "  THE/NNP\n",
    "  STATE/NNP\n",
    "  OF/NNP\n",
    "  THE/NNP\n",
    "  UNION/NNP\n",
    "  January/NNP)\n",
    "(Chunk PRESIDENT/NNP)\n",
    "(Chunk Thank/NNP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full code for this would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(S\n",
      "  (Chunk Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  (Chunk called/VBD America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(S\n",
      "  Tonight/NN\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  hope/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  glad/JJ\n",
      "  reunion/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  husband/NN\n",
      "  who/WP\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  so/RB\n",
      "  long/RB\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  grateful/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  good/JJ\n",
      "  life/NN\n",
      "  of/IN\n",
      "  (Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "  ./.)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n",
      "(Chunk Applause/NNP)\n",
      "(S\n",
      "  (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  reacts/VBZ\n",
      "  to/TO\n",
      "  applause/VB\n",
      "  during/IN\n",
      "  his/PRP$\n",
      "  (Chunk State/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chunk Capitol/NNP)\n",
      "  ,/,\n",
      "  (Chunk Tuesday/NNP)\n",
      "  ,/,\n",
      "  (Chunk Jan/NNP)\n",
      "  ./.)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get particular enough, you may find that you may be better off if there was a way to chunk everything, except some stuff. This process is what is known as chinking, and that's what we're going to be covering next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
